
@article{noauthor_notitle_nodate,
}

@misc{ishii_cutdepthedge-aware_2021,
	title = {{CutDepth}:{Edge}-aware {Data} {Augmentation} in {Depth} {Estimation}},
	shorttitle = {{CutDepth}},
	url = {http://arxiv.org/abs/2107.07684},
	abstract = {It is difficult to collect data on a large scale in a monocular depth estimation because the task requires the simultaneous acquisition of RGB images and depths. Data augmentation is thus important to this task. However, there has been little research on data augmentation for tasks such as monocular depth estimation, where the transformation is performed pixel by pixel. In this paper, we propose a data augmentation method, called CutDepth. In CutDepth, part of the depth is pasted onto an input image during training. The method extends variations data without destroying edge features. Experiments objectively and subjectively show that the proposed method outperforms conventional methods of data augmentation. The estimation accuracy is improved with CutDepth even though there are few training data at long distances.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Ishii, Yasunori and Yamashita, Takayoshi},
	month = jul,
	year = {2021},
	note = {arXiv:2107.07684 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:files/102/2107.html:text/html;Full Text PDF:files/103/Ishii and Yamashita - 2021 - CutDepthEdge-aware Data Augmentation in Depth Est.pdf:application/pdf},
}

@inproceedings{sergey_line2depth_2021,
	address = {Online Streaming, --- Select a Country ---},
	title = {Line2depth: {Indoor} {Depth} {Estimation} from {Line} {Drawings}:},
	isbn = {978-989-758-488-6},
	shorttitle = {Line2depth},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010245104780483},
	doi = {10.5220/0010245104780483},
	language = {en},
	urldate = {2023-06-06},
	booktitle = {Proceedings of the 16th {International} {Joint} {Conference} on {Computer} {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Sergey, Pavlov and Yoshihiro, Kanamori and Yuki, Endo},
	year = {2021},
	pages = {478--483},
	file = {Sergey et al. - 2021 - Line2depth Indoor Depth Estimation from Line Draw.pdf:files/104/Sergey et al. - 2021 - Line2depth Indoor Depth Estimation from Line Draw.pdf:application/pdf},
}

@misc{wu_toward_2022,
	title = {Toward {Practical} {Monocular} {Indoor} {Depth} {Estimation}},
	url = {http://arxiv.org/abs/2112.02306},
	abstract = {The majority of prior monocular depth estimation methods without groundtruth depth guidance focus on driving scenarios. We show that such methods generalize poorly to unseen complex indoor scenes, where objects are cluttered and arbitrarily arranged in the near field. To obtain more robustness, we propose a structure distillation approach to learn knacks from an off-the-shelf relative depth estimator that produces structured but metric-agnostic depth. By combining structure distillation with a branch that learns metrics from left-right consistency, we attain structured and metric depth for generic indoor scenes and make inferences in real-time. To facilitate learning and evaluation, we collect SimSIN, a dataset from simulation with thousands of environments, and UniSIN, a dataset that contains about 500 real scan sequences of generic indoor environments. We experiment in both sim-to-real and real-to-real settings, and show improvements, as well as in downstream applications using our depth maps. This work provides a full study, covering methods, data, and applications aspects.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Wu, Cho-Ying and Wang, Jialiang and Hall, Michael and Neumann, Ulrich and Su, Shuochen},
	month = mar,
	year = {2022},
	note = {arXiv:2112.02306 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2022},
	file = {arXiv.org Snapshot:files/108/2112.html:text/html;Full Text PDF:files/109/Wu et al. - 2022 - Toward Practical Monocular Indoor Depth Estimation.pdf:application/pdf},
}

@misc{noauthor_optica_nodate,
	title = {Optica {Publishing} {Group}},
	url = {https://opg.optica.org/view_article.cfm?pdfKey=7626454c-fe6d-4f71-a8761891429e5652_458597},
	urldate = {2023-06-07},
	file = {Optica Publishing Group:files/111/view_article.html:text/html},
}

@misc{adminsvr_3dof_2021,
	title = {{3DoF} vs {6DoF} in {VR}: {What} is the difference?},
	shorttitle = {{3DoF} vs {6DoF} in {VR}},
	url = {https://www.smartvrlab.nl/3dof-vs-6dof-in-vr/},
	abstract = {The difference between 3DoF vs 6DoF is that in 6DoF you can track both rotational and transitional axes. While in 3DoF you can only track one.},
	language = {en-US},
	urldate = {2023-06-08},
	journal = {Smart VR Lab},
	author = {adminsvr},
	month = mar,
	year = {2021},
	file = {Snapshot:files/114/3dof-vs-6dof-in-vr.html:text/html},
}

@article{bogdanova_depth_2016,
	title = {Depth {Perception} of {Surgeons} in {Minimally} {Invasive} {Surgery}},
	volume = {23},
	issn = {1553-3506},
	url = {https://doi.org/10.1177/1553350616639141},
	doi = {10.1177/1553350616639141},
	abstract = {Minimally invasive surgery (MIS) poses visual challenges to the surgeons. In MIS, binocular disparity is not freely available for surgeons, who are required to mentally rebuild the 3-dimensional (3D) patient anatomy from a limited number of monoscopic visual cues. The insufficient depth cues from the MIS environment could cause surgeons to misjudge spatial depth, which could lead to performance errors thus jeopardizing patient safety. In this article, we will first discuss the natural human depth perception by exploring the main depth cues available for surgeons in open procedures. Subsequently, we will reveal what depth cues are lost in MIS and how surgeons compensate for the incomplete depth presentation. Next, we will further expand our knowledge by exploring some of the available solutions for improving depth presentation to surgeons. Here we will review the innovative approaches (multiple 2D camera assembly, shadow introduction) and devices (3D monitors, head-mounted devices, and auto-stereoscopic monitors) for 3D image presentation from the past few years.},
	language = {en},
	number = {5},
	urldate = {2023-06-08},
	journal = {Surg Innov},
	author = {Bogdanova, Rositsa and Boulanger, Pierre and Zheng, Bin},
	month = oct,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {515--524},
}

@article{bogdanova_depth_2016-1,
	title = {Depth {Perception} of {Surgeons} in {Minimally} {Invasive} {Surgery}},
	volume = {23},
	issn = {1553-3506},
	url = {https://doi.org/10.1177/1553350616639141},
	doi = {10.1177/1553350616639141},
	abstract = {Minimally invasive surgery (MIS) poses visual challenges to the surgeons. In MIS, binocular disparity is not freely available for surgeons, who are required to mentally rebuild the 3-dimensional (3D) patient anatomy from a limited number of monoscopic visual cues. The insufficient depth cues from the MIS environment could cause surgeons to misjudge spatial depth, which could lead to performance errors thus jeopardizing patient safety. In this article, we will first discuss the natural human depth perception by exploring the main depth cues available for surgeons in open procedures. Subsequently, we will reveal what depth cues are lost in MIS and how surgeons compensate for the incomplete depth presentation. Next, we will further expand our knowledge by exploring some of the available solutions for improving depth presentation to surgeons. Here we will review the innovative approaches (multiple 2D camera assembly, shadow introduction) and devices (3D monitors, head-mounted devices, and auto-stereoscopic monitors) for 3D image presentation from the past few years.},
	language = {en},
	number = {5},
	urldate = {2023-06-08},
	journal = {Surg Innov},
	author = {Bogdanova, Rositsa and Boulanger, Pierre and Zheng, Bin},
	month = oct,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {515--524},
	file = {SAGE PDF Full Text:files/116/Bogdanova et al. - 2016 - Depth Perception of Surgeons in Minimally Invasive.pdf:application/pdf},
}
